{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a4c502",
   "metadata": {},
   "source": [
    "# Engaging Complexity: Introduction to Information Theory\n",
    "\n",
    "<table style=\"background-color: white;\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"binary_symmetric.png\" alt=\"binary symmetric channel\" style=\"width: 80%;\"/>\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"Shannon.jpeg\" alt=\"Claude Shannon\" style=\"width: 120%;\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9a989897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/james/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/james/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/james/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5becd",
   "metadata": {},
   "source": [
    "# Topic 1: Probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6482722",
   "metadata": {},
   "source": [
    "## 1. What is a probability distribution?\n",
    "\n",
    "A probability distribution is a representation of how likely it is that a variable can take a particular value, for all possible values that the variable can take. There are two types of probability distribution:\n",
    "\n",
    "* A discrete probability distribution is defined in a variable that can take a finite number of possible states\n",
    "* A continuous probability distribution is defined an a variable that can take an infinte number os possible states.\n",
    "\n",
    "A probability can take a value between 0 and 1 inclusive. A value of 0 means that it is impossible for a variable to take that state; a value of 1 means that taking that state is guaranteed. Because a probability distribution is defined across all possible states of a variable, its values must sum to 1 (i.e. it is guaranteed that the system will be in <i>some</i> state)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "898d4232",
   "metadata": {},
   "source": [
    "### Example 1: A discrete probability distribution\n",
    "\n",
    "<img src=\"NUTS.png\" alt=\"UK NUTS regions\" style=\"width: 40%;\"/>\n",
    "\n",
    "Let's imagine we're interested we pick a random resident of the UK. There are only a finite number of regions they can be from. Once we know the population of each region, we can work out the probility that they are resident in that region by dividing by the regional population by the total UK population:\n",
    "\n",
    "$$P(resident\\;in\\;region) = \\frac{regional\\;population}{UK\\;population}$$\n",
    "\n",
    "According to the Office of National Statistics, the UK population is 67,025,542. Given regional populations, this gives the following distribution:\n",
    "\n",
    "\n",
    "| Region | Population | Probability |\n",
    "|:---------|:--------:|:--------:|\n",
    "| Scotland   |  5463992   | 0.081|\n",
    "| North East  |  2673468   |  0.039|\n",
    "|  North West  |  7355476   | 0.109|\n",
    "|  Yorkshire and<br>the Humber  | 5517920  | 0.082 |\n",
    "|  Northern<br>Ireland | 1898785 | 0.028 |\n",
    "|  East<br>Midlands | 4861236 | 0.072 |\n",
    "|  West<br>Midlands | 5962551| 0.088 |\n",
    "|  East of<br>England | 6259318| 0.093 |\n",
    "|  London| 9004875| 0.134 |\n",
    "|  South East| 9212113| 0.137 |\n",
    "|  South West | 5660791|0.084 |\n",
    "|  Wales | 3155017|0.047 |\n",
    "\n",
    "Charting this on a barplot gives us a graphical representation of our discrete probability distribution:\n",
    "\n",
    "<img src=\"populations_.png\" alt=\"chart\" width=\"600\" height=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733b5c6",
   "metadata": {},
   "source": [
    "## Example 2: A continuous probability distribution\n",
    "\n",
    "Let's imagine we're interested in the probability of a random person being a partcular height. While there are physical limitations on how tall a human being can be (the record is 272cm), every human has been every value of height smaller than their current height. This means that height varies continuously. Height also forms a normal distribution, which means that extreme values are uncommon.\n",
    "\n",
    "<img src=\"Height.png\" alt=\"height\" style=\"width: 60%;\"/>\n",
    "\n",
    "Though we will be dealing with discrete distributions, are two features of continuous probability distributions you should probably be aware of:\n",
    "\n",
    "* Where the sum of proabilities in a discrete distribution is 1, the area under the curve in a continuus distributions sums to 1.\n",
    "* Because a continuous distribution technically contains an infinite number of values, the probability of any one exact value occurring is 0. Instead, probability is defined in a range of values––see below.\n",
    "\n",
    "<img src=\"Height_strip.png\" alt=\"height\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e70dc3",
   "metadata": {},
   "source": [
    "## 2. Joint probability distributions\n",
    "\n",
    "A joint probability is defined across the states of two variables. It registers the probability of every possible pair of states of the two variables occurring. Let's explore with an example.\n",
    "\n",
    "Every resident of the UK lives in a specfic region; we have already worked out the probability of a random person living in each of these regions. However, every UK resident also has an ethnicity. Census data records these as follows:\n",
    "\n",
    "| Ethnicity | Population | Probability |\n",
    "|:---------|:--------:|:--------:|\n",
    "| Asian   |  5026915.6  | 0.075|\n",
    "| Black  |  2211842.8   |  0.033|\n",
    "|  Mixed  |  1474561.9   | 0.022|\n",
    "|  Other  | 670255.4  | 0.01 |\n",
    "|  White | 57641966.1 | 0.86 |\n",
    "\n",
    "\n",
    "A joint probability distribution across region of residence ($X$) and ethnicity ($Y$) would list the probility of random person being of a specific ethnicity and living in a specific region, for all values of region and ethnicity. How could be calculate this? \n",
    "\n",
    "This is done using the law of $AND$, or the multiplication rule. Namely, for any two independent events, the probability of their joint occurence is the product of their individual probabilities. (The law of $OR$ states that the probability of one OR another independent events occurring is ascertained by adding their probabilities.)\n",
    "\n",
    "Let's take an example: What's the probability of residing in the North West and being of Asian ethnicity?\n",
    "\n",
    "$$P(X=North\\;West)=0.109$$\n",
    "$$P(Y=Asian)=0.075$$\n",
    "$$P(Y=North\\;West,\\;Y=Asian)=0.109\\times{0.075}=0.008175$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2d1b4ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67025542"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ea1f6",
   "metadata": {},
   "source": [
    "### Let's create a dataframe that gives us the joint probabilities for all pairs of values for region and ethnicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "64aacbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_regions = {'Scotland': 5463992, 'North East': 2673468, 'North West': 7355476, 'Yorkshire and the Humber': 5517920, \\\n",
    "              'Northern Ireland': 1898785, 'East Midlands': 4861236, 'West Midlands': 5962551, \\\n",
    "              'East of England' : 6259318, 'London': 9004875, 'South East': 9212113, 'South West': 5660791, \\\n",
    "              'Wales': 3155017}\n",
    "total_pop = sum([i for i in UK_regions.values()])\n",
    "region_prob = [i/total_pop for i in UK_regions.values()]\n",
    "\n",
    "UK_ethnicity = {'Asian': 0.075, 'Black': 0.033, 'Mixed': 0.022, 'Other': 0.01, 'White': 0.86}\n",
    "ethnic_prob = list(UK_ethnicity.values())\n",
    "\n",
    "\n",
    "table = [[] for x in range(len(UK_regions.keys()))]\n",
    "\n",
    "for i in range(len(UK_regions.keys())):\n",
    "    for j in ethnic_prob:\n",
    "        table[i].append(region_prob[i]*j)\n",
    "    \n",
    "\n",
    "joint_distribution = pd.DataFrame(table, columns = UK_ethnicity.keys(), index = UK_regions.keys())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263911b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409a970",
   "metadata": {},
   "source": [
    "## In-class exercise\n",
    "\n",
    "1. What day of the week were you born on? [Go here to find out](https://www.timeanddate.com/date/weekday.html)\n",
    "2. Is the day of the month you were born in an odd or even number?\n",
    "\n",
    "Now, let's create a dataframe with these values for the whole class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "733b1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = {'Monday': 2, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 3, 'Friday': 3, 'Saturday': 2, 'Sunday': 5}\n",
    "oddness = {'Odd': 16, 'Even': 10}\n",
    "\n",
    "total = sum([i for i in days.values()])\n",
    "days_prob = [i/total for i in days.values()]\n",
    "oddness_prob = [i/total for i in oddness.values()]\n",
    "\n",
    "table = [[] for x in range(len(days.keys()))]\n",
    "\n",
    "for i in range(len(days.keys())):\n",
    "    for j in oddness_prob:\n",
    "        table[i].append(days_prob[i]*j)\n",
    "    \n",
    "\n",
    "class_distribution = pd.DataFrame(table, columns = oddness.keys(), index = days.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182642b",
   "metadata": {},
   "source": [
    "#### Some questions:\n",
    "\n",
    "1. What is the probability of a person having an odd birthday on a weekend?\n",
    "2. Select two people at random. What's the probability they both share a non-weekend birthday OR an odd birthday?\n",
    "3. Select two people at random. What's the probability they both share a the same day of the week as a birthday?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090191e",
   "metadata": {},
   "source": [
    "# Topic 2: Shannon entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9695b5",
   "metadata": {},
   "source": [
    "### Surprise\n",
    "\n",
    "What is entropy? There are several kinds, but the two most common are <b>thermodynamic entropy</b> and <b>information-theoretic entropy</b>. Thermyodynamic entropy is a <b>measure of disorder</b> in a system; information-theoretic entropy (called Shannon entropy) is a <b>measure of unpredicatbility</b> in a system. Though both concepts are related, we are here interested in Shannon entropy. \n",
    "\n",
    "The first step in understanding entropy comes with defining the notion of <b>surprise</b> (sometimes called surprisal, information, or self-information). When is an event surprising? When it's unlikely but happens anyway. Therefore, surprise is inveresely proportional to probability: high probability events have low surprise (we expect them to occur) while low probability events have high surprise (we don't expect them to occur).\n",
    "\n",
    "Let's take an example. What would surprise us most if it fell from the sky––frogs, ash, snow, or rain?\n",
    "\n",
    "<img src=\"frogs.png\" alt=\"frogs\" style=\"width: 60%;\"/>\n",
    "\n",
    "The answer, obviously, is frogs––but how can we quantify this? We need a function that, in the $0$ to $1$ range, makes small probabilities large and large probabilities small. Do we have such a function?\n",
    "\n",
    "Yes we do! The negative of the logarithmic function does exactly this. That is:\n",
    "\n",
    "$$S = -\\log_{2}(p)$$\n",
    "\n",
    "If we define surprise in this way and plot the results, we can quickly see why it works:\n",
    "\n",
    "<img src=\"surprise.png\" alt=\"surprise\" style=\"width: 60%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e70d1",
   "metadata": {},
   "source": [
    "### Expected value\n",
    "\n",
    "The second step in understanding entropy comes with the idea of <b>expected value</b>. This captures the long-term output of a system. It is calculated by mutiplying the probability of system's state by the output of the system's state, and adding the results for all the states. \n",
    "\n",
    "Imagine my system, $X$ is a biased coin, with $P(heads) = 0.75$ and $P(tails) = 0.25$. Now, further suppose that I get £25 every time I get a head, and £35 every time I get a tail. My exepcted value for $X$, $E[X]$ is given by:\n",
    "\n",
    "$$E[X] = (0.75\\times{25})+(0.25\\times{35})= £27.5$$\n",
    "\n",
    "That is, the long-run average of my takings will trend towards £27.5. We can see this by simulating 1,000 trials:\n",
    "\n",
    "<img src=\"expected_value_.png\" alt=\"ev\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071b99f",
   "metadata": {},
   "source": [
    "### Definition: Entropy is the expected value of surprise\n",
    "\n",
    "We can now attempt to define Shannon entropy, often denoted by $H$. Entropy is a measure of unpredictability and therefore of surprise; we have already seen how $-log_{2}(p)$ quantifies surprise for a single event. In entropy, we are interested in the surprise associated with all the states (i.e. events) of a specfic system. In other words, <b>entropy is the expected value of surprise</b>. To calculate it, we multiply the surprise of a state by its probability of occurring, and add up the results for every state of the system. \n",
    "\n",
    "This is the formula for entropy, where $H$ is the entropy measure and $X$ is a discrete probability distribution over $n$ states of a system:\n",
    "\n",
    "$$X = (x_1, x_2, x_3, ... x_n)$$\n",
    "\n",
    "$$H(X) = -\\sum_{x \\in X} p(x)\\log_{2} p(x)$$\n",
    "\n",
    "The $\\sum_{x \\in X}$ notation here simply means \"add up the results for every value of $x$ in $X$\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84dbbb9",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "<img src=\"urn.png\" alt=\"ev\" style=\"width: 30%;\"/>\n",
    "\n",
    "\n",
    "What does this look like in practice? Let's take an example. An urn contains 7 red balls and 3 blue balls. What is the entropy of this system $X$? The answer comes with recognising that it defines a probability distribution across colours:\n",
    "\n",
    "$P(X=red):\\frac{7}{10}$\n",
    "\n",
    "$P(X=blue):\\frac{3}{10}$\n",
    "\n",
    "$$H(X) = -\\begin{bmatrix}{\\frac{7}{10}\\times \\log_{2}(\\frac{7}{10}) + \\frac{3}{10}\\times \\log_{2}(\\frac{3}{10})}\\end{bmatrix}$$\n",
    "\n",
    "$$H(X) = -[-0.5210896782498619 - 0.3602012209808308]$$\n",
    "\n",
    "$$H(X) = 0.8812908992306927\\;bits$$\n",
    "\n",
    "Compare with an urn that contains 5 red balls and 5 blue balls:\n",
    "\n",
    "$$H(X) = -\\begin{bmatrix}{\\frac{5}{10}\\times \\log_{2}(\\frac{5}{10}) + \\frac{5}{10}\\times \\log_{2}(\\frac{5}{10})}\\end{bmatrix}$$\n",
    "\n",
    "$$H(X) = -[-0.5 - 0.5]$$\n",
    "\n",
    "$$H(X) = 1\\;bits$$\n",
    "\n",
    "Because the urn with equal numbers of red and blue balls is harder to predict, it has higher entropy than the system with more red than blue balls. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a874a",
   "metadata": {},
   "source": [
    "### Calculating entropy using `scipy`\n",
    "\n",
    "Usefully, python allows us to calculate entropy easily using the `scipy` library. [Find the docs here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "45ee84b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of probability distribution of residing in UK region in bits: 2.4038891997141607\n",
      "Entropy of probability distribution of belonging to ethnicity in bits: 0.5665682812818006\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "regions_e = entropy(region_prob)\n",
    "ethnic_e = entropy(ethnic_prob)\n",
    "\n",
    "print(\"Entropy of probability distribution of residing in UK region in bits:\", regions_e)\n",
    "print(\"Entropy of probability distribution of belonging to ethnicity in bits:\", ethnic_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694505b",
   "metadata": {},
   "source": [
    "# Topic 3: Problem-solving using entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550090c",
   "metadata": {},
   "source": [
    "### 1. Distinguishing between language and noise\n",
    "\n",
    "One particular value of entropy is in distinguishing between in information containing signal and noise. For a signal to be present, it must be encoded––and encoding implies repetition. Here, we will explore an example of how we might use entropy to do this for written text.\n",
    "\n",
    "1. We we will split two texts––Emily Brontë's <i>Wuthering Heights</i> and William Shakespeare's <i>Collected Works</i>––into their constituent words.\n",
    "2. We will generate 25 texts made up of random strings of characters of characters of random length.\n",
    "3. We will cryptographically hash each word of each text using the SHA-256 algorithm to make them indistinguishable to the human eye.\n",
    "4. We will scramle the list of texts so we don't know where the real and the fake ones are.\n",
    "5. We will measure the entropy of the resulting hashed word frequency distributions to see if we can identify the real texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and demo the hashing library\n",
    "\n",
    "import hashlib\n",
    "\n",
    "input_string = \"Hello, world!\"\n",
    "\n",
    "# Create a SHA-256 hash object\n",
    "hash_object = hashlib.sha256()\n",
    "hash_object.update(input_string.encode())\n",
    "\n",
    "hex_dig = hash_object.hexdigest()\n",
    "\n",
    "print(hex_dig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "80a8de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open and extract words from our literary texts\n",
    "\n",
    "with open('wuthering_heights.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = text.encode('ascii', 'ignore')\n",
    "text = text.decode()\n",
    "text = ' '.join(text.splitlines())\n",
    "text = text.lower()\n",
    "words = word_tokenize(text)\n",
    "lemmas_wh = [lemmatizer.lemmatize(i) for i in words]\n",
    "\n",
    "with open('shakespeare_cw.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = text.encode('ascii', 'ignore')\n",
    "text = text.decode()\n",
    "text = ' '.join(text.splitlines())\n",
    "text = text.lower()\n",
    "words = word_tokenize(text)\n",
    "lemmas_sh = [lemmatizer.lemmatize(i) for i in words]\n",
    "lemmas_sh = lemmas_sh[:len(lemmas_wh)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "32a05125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate our noisy texts\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "def randoms(n, min_length, max_length):\n",
    "    random_strings = []\n",
    "    for _ in range(n):\n",
    "        length = random.randint(min_length, max_length)\n",
    "        random_str = ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\n",
    "        random_strings.append(random_str)\n",
    "    return random_strings\n",
    "\n",
    "n = len(lemmas_wh)  # Number of strings\n",
    "min_length = 1  # Minimum length of strings\n",
    "max_length = 12  # Maximum length of strings\n",
    "\n",
    "texts = []\n",
    "\n",
    "for i in range(25):\n",
    "    texts.append(randoms(n, min_length, max_length))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "9bcecb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle and then ryptographically hash all our texts\n",
    "\n",
    "import random\n",
    "all_text = texts + [lemmas_wh] + [lemmas_sh]\n",
    "random.shuffle(all_text)\n",
    "\n",
    "hashes = [[] for i in range(len(all_text))]\n",
    "\n",
    "for i in range(len(all_text)):\n",
    "    for j in all_text[i]:\n",
    "        hash_object = hashlib.sha256()\n",
    "        hash_object.update(j.encode())\n",
    "        hex_dig = hash_object.hexdigest()\n",
    "        hashes[i].append(hex_dig)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7029ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get counts of hashed words and create probability distribution\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "distributions = []\n",
    "\n",
    "for i in hashes:\n",
    "    item_counts = Counter(i)\n",
    "    prob = [j/len(i) for j in item_counts.values()]\n",
    "    distributions.append(prob)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7841a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate and plot entropies for each text\n",
    "\n",
    "entropies = [entropy(i, base = 2) for i in distributions]\n",
    "entropies = pd.Series(entropies)\n",
    "\n",
    "sns.barplot(x = entropies.index, y = entropies.values)\n",
    "plt.xlabel('hashed text ID')\n",
    "plt.ylabel('bits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe5b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
