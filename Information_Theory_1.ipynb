{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a4c502",
   "metadata": {},
   "source": [
    "# Engaging Complexity: Introduction to Information Theory\n",
    "\n",
    "<table style=\"background-color: white;\">\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"binary_symmetric.png\" alt=\"binary symmetric channel\" style=\"width: 80%;\"/>\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"Shannon.jpeg\" alt=\"Claude Shannon\" style=\"width: 120%;\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a989897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "#!pip install ipywidgets\n",
    "import ipywidgets\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from math import comb\n",
    "from collections import Counter\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5becd",
   "metadata": {},
   "source": [
    "# Topic 1: Probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6482722",
   "metadata": {},
   "source": [
    "## 1. What is a probability distribution?\n",
    "\n",
    "A probability distribution is a representation of how likely it is that a variable can take a particular value, for all possible values that the variable can take. There are two types of probability distribution:\n",
    "\n",
    "* A discrete probability distribution is defined on a variable that can take a finite number of possible states\n",
    "* A continuous probability distribution is defined on a variable that can take an infinte number of possible states.\n",
    "\n",
    "A probability can take a value between 0 and 1 inclusive. A value of 0 means that it is impossible for a variable to take that state; a value of 1 means that taking that state is guaranteed. Because a probability distribution is defined across all possible states of a variable, its values must sum to 1 (i.e. it is guaranteed that the system will be in <i>some</i> state)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d4232",
   "metadata": {},
   "source": [
    "### Example 1: A discrete probability distribution\n",
    "\n",
    "<img src=\"NUTS.png\" alt=\"UK NUTS regions\" style=\"width: 40%;\"/>\n",
    "\n",
    "Let's imagine we're interested in picking a random resident of the UK. There are only a finite number of regions they can be resident in at a specific point in time. Once we know the population of each region, we can work out the probability that they are resident in that region by dividing by the regional population by the total UK population:\n",
    "\n",
    "$$P(resident\\;in\\;region) = \\frac{regional\\;population}{UK\\;population}$$\n",
    "\n",
    "According to the Office of National Statistics, the UK population is 67,025,542. Given regional populations, this gives the following distribution:\n",
    "\n",
    "\n",
    "| Region | Population | Probability |\n",
    "|:---------|:--------:|:--------:|\n",
    "| Scotland   |  5463992   | 0.081|\n",
    "| North East  |  2673468   |  0.039|\n",
    "|  North West  |  7355476   | 0.109|\n",
    "|  Yorkshire and<br>the Humber  | 5517920  | 0.082 |\n",
    "|  Northern<br>Ireland | 1898785 | 0.028 |\n",
    "|  East<br>Midlands | 4861236 | 0.072 |\n",
    "|  West<br>Midlands | 5962551| 0.088 |\n",
    "|  East of<br>England | 6259318| 0.093 |\n",
    "|  London| 9004875| 0.134 |\n",
    "|  South East| 9212113| 0.137 |\n",
    "|  South West | 5660791|0.084 |\n",
    "|  Wales | 3155017|0.047 |\n",
    "\n",
    "Charting this on a barplot gives us a graphical representation of our discrete probability distribution:\n",
    "\n",
    "<img src=\"populations_.png\" alt=\"chart\" width=\"600\" height=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733b5c6",
   "metadata": {},
   "source": [
    "## Example 2: A continuous probability distribution\n",
    "\n",
    "Let's imagine we're interested in the probability of a random person being a partcular height. While there are physical limitations on how tall a human being can be (the record is 272cm), every human has been every value of height smaller than their current height. This means that height varies continuously. Height also forms a normal distribution, which means that extreme values are uncommon. Allowing that different ethnicities have different average heights, the overall mean for humans is around 175cm with a standard deviation of 8cm.\n",
    "\n",
    "<img src=\"Height.png\" alt=\"height\" style=\"width: 60%;\"/>\n",
    "\n",
    "Though we will be dealing with discrete distributions, are are two features of continuous probability distributions you should probably be aware of:\n",
    "\n",
    "* Where the sum of probabilities in a discrete distribution is 1, the area under the curve in a continuus distributions sums to 1.\n",
    "* Because a continuous distribution technically contains an infinite number of values, the probability of any one exact value occurring is 0. Instead, probability is defined in a range of values––see below.\n",
    "* To get the probability of being in a particular range (which can be arbitrarily small), we caculate the area under the curve between the top and bottom of our ranges.\n",
    "\n",
    "<img src=\"Height_strip.png\" alt=\"height\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58693f-cc6d-4443-889a-ed19b550e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 175    # Mean height\n",
    "sigma = 8 #Standard deviation\n",
    "\n",
    "height_distribution = norm(loc=mu, scale=sigma)\n",
    "\n",
    "lower = 179\n",
    "higher = 181\n",
    "\n",
    "# Compute the area under the curve in our range of interest\n",
    "area = height_distribution.cdf(higher) - height_distribution.cdf(lower)\n",
    "\n",
    "print(f\"The probability of person being between {lower}cm and {higher}cm is {area}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e70dc3",
   "metadata": {},
   "source": [
    "## 2. Joint probability distributions\n",
    "\n",
    "A joint probability is defined across the states of two variables. It registers the probability of every possible pair of states of the two variables occurring. Let's explore with an example.\n",
    "\n",
    "Every resident of the UK lives in a specfic region; we have already worked out the probability of a random person living in each of these regions. However, every UK resident also has an ethnicity. Census data records these as follows:\n",
    "\n",
    "| Ethnicity | Population | Probability |\n",
    "|:---------|:--------:|:--------:|\n",
    "| Asian   |  5026915.6  | 0.075|\n",
    "| Black  |  2211842.8   |  0.033|\n",
    "|  Mixed  |  1474561.9   | 0.022|\n",
    "|  Other  | 670255.4  | 0.01 |\n",
    "|  White | 57641966.1 | 0.86 |\n",
    "\n",
    "\n",
    "A joint probability distribution across region of residence ($X$) and ethnicity ($Y$) would list the probility of random person being of a specific ethnicity and living in a specific region, for all values of region and ethnicity. How could we calculate this? \n",
    "\n",
    "This is done using the law of $AND$, or the multiplication rule. Namely, for any two independent events, the probability of their joint occurence is the product of their individual probabilities. (The law of $OR$ states that the probability of one OR another independent events occurring is ascertained by adding their probabilities.)\n",
    "\n",
    "Let's take an example: What's the probability of residing in the North West and being of Asian ethnicity?\n",
    "\n",
    "$$P(X=North\\;West)=0.109$$\n",
    "$$P(Y=Asian)=0.075$$\n",
    "$$P(Y=North\\;West,\\;Y=Asian)=0.109\\times{0.075}=0.008175$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ea1f6",
   "metadata": {},
   "source": [
    "### Let's create a dataframe that gives us the joint probabilities for all pairs of values for region and ethnicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aacbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_regions = {'Scotland': 5463992, 'North East': 2673468, 'North West': 7355476, 'Yorkshire and the Humber': 5517920, \\\n",
    "              'Northern Ireland': 1898785, 'East Midlands': 4861236, 'West Midlands': 5962551, \\\n",
    "              'East of England' : 6259318, 'London': 9004875, 'South East': 9212113, 'South West': 5660791, \\\n",
    "              'Wales': 3155017}\n",
    "total_pop = sum([i for i in UK_regions.values()])\n",
    "region_prob = [i/total_pop for i in UK_regions.values()]\n",
    "\n",
    "UK_ethnicity = {'Asian': 0.075, 'Black': 0.033, 'Mixed': 0.022, 'Other': 0.01, 'White': 0.86}\n",
    "ethnic_prob = list(UK_ethnicity.values())\n",
    "\n",
    "\n",
    "table = [[] for x in range(len(UK_regions.keys()))]\n",
    "\n",
    "for i in range(len(UK_regions.keys())):\n",
    "    for j in ethnic_prob:\n",
    "        table[i].append(region_prob[i]*j)\n",
    "    \n",
    "\n",
    "joint_distribution = pd.DataFrame(table, columns = UK_ethnicity.keys(), index = UK_regions.keys())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95147aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409a970",
   "metadata": {},
   "source": [
    "## In-class exercise\n",
    "\n",
    "1. What day of the week were you born on? [Go here to find out](https://www.timeanddate.com/date/weekday.html)\n",
    "2. Is the day of the month you were born in an odd or even number?\n",
    "\n",
    "Now, let's create a dataframe with these values for the whole class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = {'Monday': 2, 'Tuesday': 5, 'Wednesday': 6, 'Thursday': 3, 'Friday': 3, 'Saturday': 2, 'Sunday': 5}\n",
    "oddness = {'Odd': 16, 'Even': 10}\n",
    "\n",
    "total = sum([i for i in days.values()])\n",
    "days_prob = [i/total for i in days.values()]\n",
    "oddness_prob = [i/total for i in oddness.values()]\n",
    "\n",
    "table = [[] for x in range(len(days.keys()))]\n",
    "\n",
    "for i in range(len(days.keys())):\n",
    "    for j in oddness_prob:\n",
    "        table[i].append(days_prob[i]*j)\n",
    "    \n",
    "\n",
    "class_distribution = pd.DataFrame(table, columns = oddness.keys(), index = days.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c87c3c-7d75-4db0-ace5-2697d316272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182642b",
   "metadata": {},
   "source": [
    "#### Some questions:\n",
    "\n",
    "1. What is the probability of a person having an odd birthday on a weekend?\n",
    "2. Select two people at random. What's the probability they both share a non-weekend birthday OR an odd birthday?\n",
    "3. Select two people at random. What's the probability they both share a the same day of the week as a birthday?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090191e",
   "metadata": {},
   "source": [
    "# Topic 2: Shannon entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9695b5",
   "metadata": {},
   "source": [
    "### Surprise\n",
    "\n",
    "What is entropy? There are several kinds, but the two most common are <b>thermodynamic entropy</b> and <b>information-theoretic entropy</b>. Thermyodynamic entropy is a <b>measure of disorder</b> in a system; information-theoretic entropy (called Shannon entropy) is a <b>measure of unpredicatbility</b> in a system. Though both concepts are related, we are here interested in Shannon entropy. \n",
    "\n",
    "The first step in understanding entropy comes with defining the notion of <b>surprise</b> (sometimes called surprisal, information, or self-information). When is an event surprising? When it's unlikely but happens anyway. Therefore, surprise is inveresely proportional to probability: high probability events have low surprise (we expect them to occur) while low probability events have high surprise (we don't expect them to occur).\n",
    "\n",
    "Let's take an example. What would surprise us most if it fell from the sky––frogs, ash, snow, or rain?\n",
    "\n",
    "<img src=\"frogs.png\" alt=\"frogs\" style=\"width: 60%;\"/>\n",
    "\n",
    "The answer, obviously, is frogs––but how can we quantify this? We need a function that, in the $0$ to $1$ range, makes small probabilities large and large probabilities small. Do we have such a function?\n",
    "\n",
    "Yes we do! The negative of the logarithmic function does exactly this. That is:\n",
    "\n",
    "$$S = -\\log_{2}(p)$$\n",
    "\n",
    "If we define surprise in this way and plot the results, we can quickly see why it works:\n",
    "\n",
    "<img src=\"surprise.png\" alt=\"surprise\" style=\"width: 60%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e70d1",
   "metadata": {},
   "source": [
    "### Expected value\n",
    "\n",
    "The second step in understanding entropy comes with the idea of <b>expected value</b>. This captures the long-term output of a system. It is calculated by mutiplying the probability of system's state by the output of the system's state, and adding the results for all the states. \n",
    "\n",
    "Imagine my system, $X$ is a biased coin, with $P(heads) = 0.75$ and $P(tails) = 0.25$. Now, further suppose that I get £25 every time I get a head, and £35 every time I get a tail. My expected value for $X$, $E[X]$ is given by:\n",
    "\n",
    "$$E[X] = (0.75\\times{25})+(0.25\\times{35})= £27.5$$\n",
    "\n",
    "That is, the long-run average of my takings will trend towards £27.5. We can see this by simulating 1,000 trials and plotting the running average:\n",
    "\n",
    "<img src=\"expected_value_.png\" alt=\"ev\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05a403-292a-42dd-bb51-8eedd4db0e3b",
   "metadata": {},
   "source": [
    "#### An exercise\n",
    "\n",
    "Imagine you have a biased dice with six faces. You get the GBP value of the face that turns up––i.e. if face 3 turns up, you get £3. Now, imagine the bias is as in the table below. What is the expected value?\n",
    "\n",
    "| Face   | Face 1 | Face 2 | Face 3 | Face 4 | Face 5 | Face 6 |\n",
    "|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-----------:|:----------:|\n",
    "| **Probability**     | $\\frac{1}{10}$| $\\frac{3}{10}$| $\\frac{1}{4}$ | $\\frac{3}{20}$| $\\frac{1}{10}$| $\\frac{1}{10}$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a6651-55a3-4318-baaf-d54870f8038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "payoff_dice = [1, 2, 3, 4, 5, 6]\n",
    "biased_dice = [1/10, 3/10, 1/4, 3/20, 1/10, 1/10]\n",
    "true_dice = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "\n",
    "def e_value(probs, payoff):\n",
    "    value = []\n",
    "    for i in range(len(probs)):\n",
    "        value.append(probs[i]*payoff[i])\n",
    "    expected_value = sum(value)\n",
    "    return expected_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf05bbf-f8a7-4280-be45-b20e688d9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_value(biased_dice, payoff_dice), e_value(true_dice, payoff_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071b99f",
   "metadata": {},
   "source": [
    "### Definition: Entropy is the expected value of surprise\n",
    "\n",
    "We can now attempt to define Shannon entropy, often denoted by $H$. Entropy is a measure of unpredictability and therefore of surprise; we have already seen how $-log_{2}(p)$ quantifies surprise for a single event. In entropy, we are interested in the surprise associated with all the states (i.e. events) of a specfic system. In other words, <b>entropy is the expected value of surprise</b>. To calculate it, we multiply the surprise of a state by its probability of occurring, and add up the results for every state of the system. \n",
    "\n",
    "This is the formula for entropy, where $H$ is the entropy measure and $X$ is a discrete probability distribution over $n$ states of a system:\n",
    "\n",
    "$$X = (x_1, x_2, x_3, ... x_n)$$\n",
    "\n",
    "$$H(X) = -\\sum_{x \\in X} p(x)\\log_{2} p(x)$$\n",
    "\n",
    "The $\\sum_{x \\in X}$ notation here simply means \"add up the results for every value of $x$ in $X$\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84dbbb9",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "<img src=\"urn.png\" alt=\"ev\" style=\"width: 30%;\"/>\n",
    "\n",
    "\n",
    "What does this look like in practice? Let's take an example. An urn contains 7 red balls and 3 blue balls. What is the entropy of this system $X$? The answer comes with recognising that it defines a probability distribution across colours:\n",
    "\n",
    "| Colour   | Blue | Red |\n",
    "|:-------------:|:-------------:|:-------------:|\n",
    "| **Probability**  | $\\frac{3}{10}$| $\\frac{7}{10}$|\n",
    "\n",
    "Plugging these values into the entropy formula then gives:\n",
    "\n",
    "\n",
    "$$H(X) = -\\begin{bmatrix}{\\frac{7}{10}\\times \\log_{2}(\\frac{7}{10}) + \\frac{3}{10}\\times \\log_{2}(\\frac{3}{10})}\\end{bmatrix}$$\n",
    "\n",
    "$$H(X) = -[-0.5210896782498619 - 0.3602012209808308]$$\n",
    "\n",
    "$$H(X) = 0.8812908992306927\\text{ bits}$$\n",
    "\n",
    "Compare with an urn that contains 5 red balls and 5 blue balls:\n",
    "\n",
    "$$H(X) = -\\begin{bmatrix}{\\frac{5}{10}\\times \\log_{2}(\\frac{5}{10}) + \\frac{5}{10}\\times \\log_{2}(\\frac{5}{10})}\\end{bmatrix}$$\n",
    "\n",
    "$$H(X) = -[-0.5 - 0.5]$$\n",
    "\n",
    "$$H(X) = 1\\text{ bits}$$\n",
    "\n",
    "Because the urn with equal numbers of red and blue balls is harder to predict, it has higher entropy than the system with more red than blue balls. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a874a",
   "metadata": {},
   "source": [
    "### Calculating entropy using `scipy`\n",
    "\n",
    "Usefully, python allows us to calculate entropy easily using the `scipy` library. [Find the docs here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "regions_e = entropy(region_prob)\n",
    "ethnic_e = entropy(ethnic_prob)\n",
    "\n",
    "print(\"Entropy of probability distribution of residing in UK region in bits:\", regions_e)\n",
    "print(\"Entropy of probability distribution of belonging to ethnicity in bits:\", ethnic_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694505b",
   "metadata": {},
   "source": [
    "# Topic 3: Problem-solving using entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550090c",
   "metadata": {},
   "source": [
    "### 1. Distinguishing between language and noise\n",
    "\n",
    "One particular value of entropy is in distinguishing between in information containing signal and noise. For a signal to be present, it must be encoded––and encoding implies repetition. Here, we will explore an example of how we might use entropy to do this for written text.\n",
    "\n",
    "1. We we will split two texts––Emily Brontë's <i>Wuthering Heights</i> and William Shakespeare's <i>Collected Works</i>––into their constituent words.\n",
    "2. We will generate 25 texts made up of random strings of characters of characters of random length.\n",
    "3. We will cryptographically hash each word of each text using the SHA-256 algorithm to make them indistinguishable to the human eye.\n",
    "4. We will scramble the list of texts so we don't know where the real and the fake ones are.\n",
    "5. We will measure the entropy of the resulting hashed word frequency distributions to see if we can identify the real texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and demo the hashing library\n",
    "\n",
    "import hashlib\n",
    "\n",
    "input_string = \"I am your naughty browsing history\"\n",
    "\n",
    "# Create a SHA-256 hash object\n",
    "hash_object = hashlib.sha256()\n",
    "hash_object.update(input_string.encode())\n",
    "\n",
    "hex_dig = hash_object.hexdigest()\n",
    "\n",
    "print(hex_dig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open and extract words from our literary texts\n",
    "\n",
    "with open('wuthering_heights.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = text.encode('ascii', 'ignore')\n",
    "text = text.decode()\n",
    "text = ' '.join(text.splitlines())\n",
    "text = text.lower()\n",
    "words = word_tokenize(text)\n",
    "lemmas_wh = [lemmatizer.lemmatize(i) for i in words]\n",
    "\n",
    "with open('shakespeare_cw.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = text.encode('ascii', 'ignore')\n",
    "text = text.decode()\n",
    "text = ' '.join(text.splitlines())\n",
    "text = text.lower()\n",
    "words = word_tokenize(text)\n",
    "lemmas_sh = [lemmatizer.lemmatize(i) for i in words]\n",
    "lemmas_sh = lemmas_sh[:len(lemmas_wh)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a05125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate our noisy texts\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "def randoms(n, min_length, max_length):\n",
    "    random_strings = []\n",
    "    for _ in range(n):\n",
    "        length = random.randint(min_length, max_length)\n",
    "        random_str = ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\n",
    "        random_strings.append(random_str)\n",
    "    return random_strings\n",
    "\n",
    "n = len(lemmas_wh)  # Number of strings\n",
    "min_length = 1  # Minimum length of strings\n",
    "max_length = 12  # Maximum length of strings\n",
    "\n",
    "texts = []\n",
    "\n",
    "for i in range(25):\n",
    "    texts.append(randoms(n, min_length, max_length))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcecb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle and then cryptographically hash all our texts\n",
    "\n",
    "import random\n",
    "all_text = texts + [lemmas_wh] + [lemmas_sh]\n",
    "random.shuffle(all_text)\n",
    "\n",
    "hashes = [[] for i in range(len(all_text))]\n",
    "\n",
    "for i in range(len(all_text)):\n",
    "    for j in all_text[i]:\n",
    "        hash_object = hashlib.sha256()\n",
    "        hash_object.update(j.encode())\n",
    "        hex_dig = hash_object.hexdigest()\n",
    "        hashes[i].append(hex_dig)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get counts of hashed words and create probability distribution\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "distributions = []\n",
    "\n",
    "for i in hashes:\n",
    "    item_counts = Counter(i)\n",
    "    prob = [j/len(i) for j in item_counts.values()]\n",
    "    distributions.append(prob)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7841a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate and plot entropies for each text\n",
    "\n",
    "entropies = [entropy(i, base = 2) for i in distributions]\n",
    "entropies = pd.Series(entropies)\n",
    "\n",
    "sns.barplot(x = entropies.index, y = entropies.values)\n",
    "plt.xlabel('hashed text ID')\n",
    "plt.ylabel('bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98f0b3",
   "metadata": {},
   "source": [
    "### 2. Joint entropy\n",
    "\n",
    "Just as we can have a joint probability distribution made up of two probability distributions, so too can we have joint entropy. This is given by the formula below, but in practice, we will usually already have the joint distribution worked out in our data.\n",
    "\n",
    "$$X = (x_1, x_2, x_3, ... x_n)$$\n",
    "\n",
    "$$Y = (y_1, y_2, y_3, ... y_n)$$\n",
    "\n",
    "$$H(X) = -\\sum_{y \\in Y}\\sum_{x \\in X} p(x,y)\\log_{2} p(x,y)$$\n",
    "\n",
    "#### Worked example: Flipping a coin at the traffic lights.\n",
    "\n",
    "<span style=\"font-size: 200%; font-weight: bold;\">Y</span>ou're an idle loafer standing at a set of traffic lights flipping a coin. Each coin flip coincides with a traffic light colour. What is the joint entropy of this system? First, let's define the two probability distributions we're working with:\n",
    "\n",
    "| Colour   | Green | Red | Amber |\n",
    "|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "| **Probability**  | $\\frac{2}{5}$| $\\frac{2}{5}$| $\\frac{1}{5}$ | \n",
    "\n",
    "| Face   | Heads | Tails |\n",
    "|:-------------:|:-------------:|:-------------:|\n",
    "| **Probability**  | $\\frac{1}{2}$| $\\frac{1}{2}$|\n",
    "\n",
    "Now, let's get the joint probability distribution:\n",
    "\n",
    "|   | Green | Red | Amber |\n",
    "|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "| **Heads**  | $\\frac{1}{5}$| $\\frac{1}{5}$| $\\frac{1}{10}$ | \n",
    "| **Tails**  | $\\frac{1}{5}$| $\\frac{1}{5}$| $\\frac{1}{10}$ | \n",
    "\n",
    "\n",
    "Next, we get the entropy for the <i>Heads</i> row and the entropy for the <i>Tails</i> row and we add them:\n",
    "\n",
    "$$H(X,Y) = -[(\\frac{1}{5}\\times \\log_2\\frac{1}{5} + \\frac{1}{5}\\times \\log_2\\frac{1}{5} + \\frac{1}{10}\\times \\log_2\\frac{1}{10}) + (\\frac{1}{5}\\times \\log_2\\frac{1}{5} + \\frac{1}{5}\\times \\log_2\\frac{1}{5} + \\frac{1}{10}\\times \\log_2\\frac{1}{10})$$\n",
    "\n",
    "$$H(X,Y) = 2.52 \\text{ bits}$$\n",
    "\n",
    "In this case, the joint entropy is the sum of the individual entropies for <i>Heads</i> and <i>Tails</i>. This means that these two systems are independent: the coin toss outcome does not depend on the traffic light colour, or vice versa. However, it is frequently the case that two systems are not independent: i.e. we can partially predict the state of one from the state of the other. In this scenario the joint entropy can be smaller than the sum of the entropies of the two distributions:\n",
    "\n",
    "$$H(X,Y)\\le H(X) + H(Y)$$\n",
    "\n",
    "Let's take a real-world example: the distribution of ages across the regions of the UK. \n",
    "\n",
    "* <b>Question: Are these distributions independent of each other?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_excel(\"pop_age.xlsx\", index_col = 0)\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pop.sum().sum()\n",
    "joint_df = pop/total\n",
    "\n",
    "prob_regions = [i/total for i in pop.sum(axis =1)]\n",
    "prob_ages = [i/total for i in pop.sum()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95016adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "sns.heatmap(joint_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_entropy = entropy(prob_regions) + entropy(prob_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bbdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The entropy of the population distribution by region is\", entropy(prob_regions), \"bits\")\n",
    "print(\"The entropy of the population distribution by age is\", entropy(prob_ages), \"bits\")\n",
    "print(\"The joint entropy is\", entropy(joint), \"bits\")\n",
    "print(\"The difference between the joint entropy and the sum of the individuals is\", sum_entropy- entropy(joint), \"bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10139b3c",
   "metadata": {},
   "source": [
    "$$H(X,Y) < H(X) + H(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6caec",
   "metadata": {},
   "source": [
    "# Topic 3: Measuring model fit\n",
    "\n",
    "> All models are wrong, but some are useful\n",
    ">\n",
    ">\n",
    "> ––<cite>George Box</cite>\n",
    "\n",
    "As a description of likelihood of the states of a system, a probability distribution is a <b>model</b> of that system. As a measure of the unpredictability of a system, so too is the entropy of a system a model of it. It is often the case that we have several models of a system and wish to judge which is the best match. This measured using what are known as [Bregman divergences](https://danmackinlay.name/notebook/bregman_divergences). In our case, we are interested in a specific divergence called the <b>Kullback-Leibler</b> or <i>$D_{KL}$</i> divergence. What is this quantity?\n",
    "\n",
    "Imagine we have true model $P(X)$. That is, $P(X)$ is a proability distribution across $X$, the states of a system. Let's suppose that $P(X)$ is a complicated distribution that it's hard to work with. If we don't need to be exact, then we can use a simpler distribution, $Q(X)$, to approximate $P(X)$. There are lots of situations where this is a useful feature. The question is, how can we measure how good a fit different candidates for $Q(X)$ might be? This is what the <i>KL</i> divergence does. Consider the two scenarios below:\n",
    "\n",
    "<img src=\"KL_.png\" alt=\"KL\" style=\"width: 60%;\"/>\n",
    "\n",
    "Compared to the top panel, the estimating distribution $Q_2{(X)}$ in the bottom panel covers more of $P(X)$ than $Q_1{(X)}$ in the top panel. Thus, for any range of values of $X$, the values of $Q_2{(X)}$ will, on average, be closer to $P(X)$ than $Q_1{(X)}$. How might we quantify this?\n",
    "\n",
    "Let's go back to our definition of entropy as the expected value of surprise. In all cases, $Q(X)$ will be more surprising than $P(X)$. This is because $Q(X)$ will always contain some error; if it was a perfect fit, we wouldn't need to use an estimation. However, the better the fit is, the less surprise $Q(X)$ will add to $P(X)$. We can calculate this by getting the surprise for every value of $X$ under $P$ and under $Q$ and subtracting $P$ from $Q$ to get the excess surprise under $Q$. (Here, $S$ denotes the excess surprise.)\n",
    "\n",
    "$$S(X=x) = [-\\log_{2}Q(x)] - [-\\log_{2}P(x)]$$\n",
    "$$S(X=x) = [-\\log_{2}Q(x) + \\log_{2}P(x)]$$\n",
    "$$S(X=x) = [\\log_{2}P(x) - \\log_{2}Q(x)]$$\n",
    "\n",
    "Multiplying by the true probability of $x$, here given by $P(x)$, then gives us the expected value, $E[S]$:\n",
    "\n",
    "$$E[S(X=x)] = P(x)[\\log_{2}P(x) - \\log_{2}Q(x)]$$\n",
    "\n",
    "Using the laws of logs, we can turn this difference (subtraction) into a quotient (division):\n",
    "\n",
    "$$E[S(X=x)] = P(x)\\log_{2}\\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "The final step comes with getting the expected value of excess surprise, $E[S]$, for all values of $X$. We do this (as with entropy) by summing across all values of $X$:\n",
    "\n",
    "\n",
    "$$D_{KL} = \\sum_{x \\in X}P(x)\\log_{2}\\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "In practice, however, we rarely need to calculate this by hand. The `scipy` `entropy` function gives back the $D_{KL}$ when you pass it two distibutions, where:\n",
    "\n",
    "* Both distributions are the same length\n",
    "* Neither distribution contains a zero value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d903660",
   "metadata": {},
   "source": [
    "## Example 1: Bringing down the house with the Kullback-Leibler divergence\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"purple_dice.png\" alt=\"dice\" style=\"width: 15;\"/>\n",
    "</p>\n",
    "\n",
    "<span style=\"font-size: 200%; font-weight: bold;\">Y</span>ou are a professional scoundrel whose favourite scam of the day is the scam in your head before the first scam of the day. You and your scoundrel pals have gained short-term access to the dice factory that supplies all the Las Vegas casinos. It can make both biased and unbiased dice; this amazing functionality exists because it is a dice factory and not, say, a bakery. You are aware of the following facts:\n",
    "\n",
    "* Pharoah's Casino will be taking its monthly order of new dice tomorrow.\n",
    "* Quality control at Pharoah's take a random selection of dice from each order and run 1000 trials to evaluate the expected value; if the expected value differs from 3.5 by more than 10%, Pharoah's rejects the order as biased.\n",
    "* In all dice games where you play against the house, the odds favour the house over the long run.\n",
    "* If your takings exceed the expected value of takings for a player of that game by 10% over 100 runs of the game, the house will suspect you of cheating and you'll end up swimming with the fishes somewhere in the Nevada desert.\n",
    "\n",
    "#### Question: How might you use your knowledge of information theory to exploit this situation?\n",
    "\n",
    "1. Create a biased order of dice that disadvantages the house but gets through quality control.\n",
    "2. Create a second batch of biased dice that approximates as closely as possible the biased house dice, but still gives you and your pals an advantage <i>without</i> triggering the suspicion of cheating––i.e. you'll win less money per winning event but you'll win more often than the house.\n",
    "\n",
    "#### Data: What have you got to work with? \n",
    "\n",
    "You've already figured out the bias for house dice that stays just within quality control; the expected value, $E[X]$, for this is 3.15––exactly 10% lower than the expected value for a fair dice.\n",
    "\n",
    "| Face   | Face 1 | Face 2 | Face 3 | Face 4 | Face 5 | Face 6 |\n",
    "|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-----------:|:----------:|\n",
    "| **Probability**     | $\\frac{1}{10}$| $\\frac{3}{10}$| $\\frac{1}{4}$ | $\\frac{3}{20}$| $\\frac{1}{10}$| $\\frac{1}{10}$|\n",
    "\n",
    "\n",
    "#### Constraints: What practical difficulties must you overcome?\n",
    "\n",
    "You don't have much time in the dice factory, and have to produce enough dice to match the whole casino order and the smaller batch for the gang to use. This means you must save time by biasing only two faces of the dice the gang will use. This means our estimating distribution, $Q$, will be of the form:\n",
    "\n",
    "| Face   | Face 1 | Face 2 | Face 3 | Face 4 | Face 5 | Face 6 |\n",
    "|:-------------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "| **Probability**     | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $\\frac{1}{6}$ | $x$ | $y$ |\n",
    "\n",
    "#### What do you do???\n",
    "\n",
    "Like any sensible scoundrel, you use the Kullback-Leibler divergence to estimate the values for $x$ and $y$. Given that $x$ and $y$ must sum to $\\frac{1}{3}$, we can get the $D_{KL}$ for all values of $p \\le{\\frac{1}{3}}$, where $x = p$ and $y = \\frac{1}{3}-p$. If these values give a higher expected value than the casino dice but lower than a fair dice, we're in business:\n",
    "\n",
    "$$E[\\text{casino}] < E[\\text{scoundrel}] \\le E[\\text{fair}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_probs = [1/6, 1/6, 1/6, 1/6]\n",
    "Q = []\n",
    "P = [1/10, 3/10, 1/4, 3/20, 1/10, 1/10]\n",
    "\n",
    "for i in np.linspace(0.01,0.33,10000):\n",
    "    E = fixed_probs.copy()\n",
    "    E.append(i)\n",
    "    E.append(0.33-i)\n",
    "    Q.append(E)\n",
    "\n",
    "KL = [entropy(P,i, base =2) for i in Q]\n",
    "\n",
    "min_ = min(KL)\n",
    "p = Q[KL.index(min_)][-2]\n",
    "x_ = p\n",
    "y_ = 0.33-p\n",
    "\n",
    "sns.lineplot(x = np.linspace(0.01,0.33,10000), y = KL)\n",
    "plt.xlabel('bias ($p$)')\n",
    "plt.ylabel('$D_{KL}$ in bits')\n",
    "plt.title(f\"The minimum $D_{{KL}}$ divergence is {min_} bits,\\n with $p$ = {x_}\")\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a360f1c-6c00-440b-8ece-8ed8a759214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The expected value of the casino dice is {e_value(payoff_dice, P)}.\")\n",
    "print(f\"The expected value of the scoundrels' dice is {e_value(payoff_dice, [1/6, 1/6, 1/6, 1/6, x_, y_])}.\")\n",
    "print(f\"The expected value of a fair dice is {e_value(payoff_dice, [1/6, 1/6, 1/6, 1/6, 1/6, 1/6])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3039b7",
   "metadata": {},
   "source": [
    "## Example 2: Creating a therapy app for men using the Kullback-Leibler divergence \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"mental.png\" alt=\"dice\" style=\"width: 5;\"/>\n",
    "</p>\n",
    "\n",
    "<span style=\"font-size: 200%; font-weight: bold;\">Y</span>ou want to create a language model to power an app that offers mental health advice to men. This means fine tuning an existing language model on a corpus of language that is representative how men express themselves emotionally. One possible corpus [is this 2022 Twitter dataset](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification), which offers a large number of sentences tagged by gender.\n",
    "\n",
    "#### The problem: How do you assess whether the Twitter corpus captures the the emotional expressiveness of men?\n",
    "\n",
    "Linguists have compiled several large corpora that give a big picture overview of how language is used by different genders across all domains of human activity. One of these is the [Fisher corpus](https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/lrec2004-fisher-corpus.pdf), which used around 2,000 hours of recorded phone calls in the USA from the early 2000s to gain word frequency estimates. These were recorded as frequency of word per million word used, and divided by gender. The Fisher corpus thus provides a reliable 'ground truth' for gendered language use in English, at least in its North American variety. \n",
    "\n",
    "#### The implementatation: How do we use the Fisher corpus to assess the viability of our Twitter data as a fine tuning source on men's emotional expressiveness?\n",
    "\n",
    "To achieve this, we can compare the distribution of emotion words by gender in the Twitter data to their distribution in the Fisher Corpus, and use the $D_{KL}$ to quantify the difference. If the Twitter data gives a better approximation to female expressiveness, then we should look elsewhere for our male training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90775dd2",
   "metadata": {},
   "source": [
    "### 1. Import data and define emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = pd.read_csv(\"fisher_corpus.csv\", index_col = 0)\n",
    "twitter = pd.read_csv(\"twitter_gender.csv\", encoding = 'latin-1')\n",
    "\n",
    "emotions = [\n",
    "    'happiness', 'sadness', 'anger', 'fear', 'surprise', \\\n",
    "    'disgust', 'love', 'envy', 'jealousy', 'pride', 'shame', \\\n",
    "    'guilt', 'contentment', 'disappointment', 'hope', 'despair', \\\n",
    "    'curiosity', 'boredom', 'loneliness', 'gratitude', 'excitement', \n",
    "    'anxiety', 'sympathy', 'compassion', 'frustration', 'euphoria',\\\n",
    "    'melancholy', 'nostalgia', 'optimism', 'empathy', 'apathy', \\\n",
    "    'confusion', 'relief', 'indignation', 'admiration', 'amazement', \\\n",
    "    'awe', 'serenity', 'calmness', 'overwhelm', 'eagerness', 'satisfaction', \\\n",
    "    'enthusiasm', 'passion', 'infatuation', 'longing', 'regret', \\\n",
    "    'resentment', 'irritation', 'discontent', 'grief', 'sorrow', \\\n",
    "    'elation', 'glee', 'joy', 'bliss', 'delight', 'pleasure', 'humility', \\\n",
    "    'disdain', 'contempt', 'spite', 'trust', 'distrust', 'skepticism', 'faith', \\\n",
    "    'doubt', 'shock', 'horror', 'terror', 'panic', 'hysteria', 'agony', \\\n",
    "    'ecstasy', 'tranquility', 'peacefulness', 'jubilation', 'triumph', 'defeat', \\\n",
    "    'anticipation', 'yearning', 'remorse', 'acceptance', 'resignation', 'indifference', \\\n",
    "    'apprehension', 'embarrassment', 'tenderness', 'warmth'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b374c",
   "metadata": {},
   "source": [
    "### 2. Process the Twitter data into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter[['gender', 'text']]\n",
    "\n",
    "lem = []\n",
    "\n",
    "for i in twitter['text']:\n",
    "    words = word_tokenize(i)\n",
    "    lemmas = [lemmatizer.lemmatize(i) for i in words]\n",
    "    lemmas = [i.lower() for i in lemmas if i not in punct and i not in stops]\n",
    "    lem.append([i for i in lemmas])\n",
    "\n",
    "twitter['lemmas'] = lem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a19430",
   "metadata": {},
   "source": [
    "### 3. Get the distribution of emotion word use by male and female in the Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b343197",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = []\n",
    "female_words = []\n",
    "\n",
    "twitter_m = twitter[twitter['gender'] == 'male']\n",
    "twitter_m = twitter_m[['gender', 'text', 'lemmas']]\n",
    "\n",
    "twitter_f = twitter[twitter['gender'] == 'female']\n",
    "twitter_f = twitter_f[['gender', 'text', 'lemmas']]\n",
    "\n",
    "for i in twitter_m['lemmas']:\n",
    "    for j in i:\n",
    "        male_words.append(j)\n",
    "        \n",
    "for i in twitter_f['lemmas']:\n",
    "    for j in i:\n",
    "        female_words.append(j)\n",
    "        \n",
    "male_freq = Counter(male_words)\n",
    "female_freq = Counter(female_words)\n",
    "\n",
    "male_emotions = [male_freq[i] for i in emotions]\n",
    "female_emotions = [female_freq[i] for i in emotions]\n",
    "\n",
    "# Add small value to prevent zero values\n",
    "\n",
    "epsilon=1e-10\n",
    "\n",
    "m_smoothed = np.array(male_emotions) + epsilon\n",
    "f_smoothed = np.array(female_emotions) + epsilon\n",
    "\n",
    "# Normalizing the distributions\n",
    "m_normalized = m_smoothed / m_smoothed.sum()\n",
    "f_normalized = f_smoothed / f_smoothed.sum()\n",
    "\n",
    "twitter_emo = pd.DataFrame()\n",
    "twitter_emo.index = emotions\n",
    "twitter_emo['male'] = m_normalized\n",
    "twitter_emo['female'] = f_normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e35d04-0781-469d-9ff5-c9b685550fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_emo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f596ea0",
   "metadata": {},
   "source": [
    "### 4. Get frequency data for the emotion words in the Fisher corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_emo = fisher.loc[emotions]\n",
    "fisher_emo['male'] = [i for i in fisher_emo['male']/fisher_emo['male'].sum()]\n",
    "fisher_emo['female'] = [i for i in fisher_emo['female']/fisher_emo['female'].sum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741da9a1",
   "metadata": {},
   "source": [
    "### 5. Get the $D_{KL}$ for male and female emotion words in Twitter relative to Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b199b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_KL = entropy(fisher_emo['male'], twitter_emo['male'])\n",
    "female_KL = entropy(fisher_emo['female'], twitter_emo['female'])\n",
    "print(f'The male KL divergence is {male_KL} bits')\n",
    "print(f'The female KL divergence is {female_KL} bits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45546f5a-beaf-4f92-bcdd-d1541da1be2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
